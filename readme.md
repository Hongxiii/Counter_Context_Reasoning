<h1 align='center' style="text-align:center; font-weight:bold; font-size:2.0em;letter-spacing:2.0px;">
  Challenging and Enhancing the Reasoning Capacity of Multimodal LLMs in Context-violating Images
</h1>      
<p align='center' style="text-align:center;font-size:1.25em;">
    <a href="https://github.com/Tough-Stone" target="_blank" style="text-decoration: none;">Hongxi&nbsp;Li</a>,&nbsp;
    <a href="https://wuxinxiao.github.io/" target="_blank" style="text-decoration: none;">Yuyang&nbsp;Chen</a>,&nbsp;
    <a href="https://wuxinxiao.github.io/" target="_blank" style="text-decoration: none;">Yayun&nbsp;Qi</a>,&nbsp;
    <a href="https://wuxinxiao.github.io/" target="_blank" style="text-decoration: none;">Xinxiao&nbsp;Wu</a>,&nbsp;<br/>
&nbsp;Beijing Institute of Technology<br/>
<em>arXiv 2024</em><br/>
<a href="https://tough-stone.github.io/contextualbench" title="Website" target="_blank" rel="nofollow" style="text-decoration: none;">ğŸŒWebsite</a> |
<a href="https://huggingface.co/datasets/ToughStone/ContextualBench" title="Dataset" target="_blank" rel="nofollow" style="text-decoration: none;">ğŸ“šDataset</a> |
<a href="https://wuxinxiao.github.io/" title="arXiv" target="_blank" rel="nofollow" style="text-decoration: none;">ğŸ“„arXiv  (Comming soon)</a> |
<a href="https://huggingface.co/spaces/ToughStone/ContextualBench_Leaderboard" title="Leaderboard" target="_blank" rel="nofollow" style="text-decoration: none;">ğŸ† Leaderboard</a>
</p>

<p align='center'>
<img src="ours/description.png" alt="dataset description" align='center' width="850" height="200">
</p>

# 1. Project Structure
<!-- ```
â”œâ”€â”€ modelsï¼šæ¨¡å‹æ–‡ä»¶
â”œâ”€â”€ datasetsï¼šæ•°æ®é›†æ–‡ä»¶
â”‚   â”œâ”€â”€ imagesï¼šå›¾åƒ
â”‚   â””â”€â”€ annotation.xlsxï¼šæ ‡æ³¨
â”œâ”€â”€ databaseï¼šçŸ¥è¯†åº“æ–‡ä»¶
â”œâ”€â”€ resultsï¼šæ¨ç†ç»“æœæ–‡ä»¶
â”‚   â”œâ”€â”€ image_captionï¼šå›¾åƒæè¿°ç»“æœ
â”‚   â”œâ”€â”€ question_answerï¼šè§†è§‰é—®ç­”ç»“æœ
â”‚   â”œâ”€â”€ image_identificationï¼šå›¾åƒè¯†åˆ«ç»“æœ
â”‚   â””â”€â”€ image_explanationï¼šå›¾åƒè§£é‡Šç»“æœ
â”œâ”€â”€ baselineï¼šåŸºçº¿ç¨‹åº
â”‚   â”œâ”€â”€ LLaVAï¼šLLaVAå®˜æ–¹ä»£ç 
â”‚   â”œâ”€â”€ mPLUG-Owlï¼šmPLUG-Owlå®˜æ–¹ä»£ç 
â”‚   â”œâ”€â”€ mPLUG-Owl2ï¼šmPLUG-Owl2å®˜æ–¹ä»£ç 
â”‚   â”œâ”€â”€ Otterï¼šOtterå®˜æ–¹ä»£ç 
â”‚   â”œâ”€â”€ openflamingoï¼šopenflamingoå®˜æ–¹ä»£ç 
â”‚   â”œâ”€â”€ MICï¼šMMICLå®˜æ–¹ä»£ç 
â”‚   â”œâ”€â”€ llamaï¼šllamaå®˜æ–¹ä»£ç 
â”‚   â”œâ”€â”€ FastChatï¼švicunaå®˜æ–¹ä»£ç 
â”‚   â”œâ”€â”€ demoï¼šæ¼”ç¤ºæ¨ç†ä»£ç 
â”‚   â”œâ”€â”€ infer_image_caption.pyï¼šå›¾åƒæè¿°æ¨ç†ç¨‹åº
â”‚   â”œâ”€â”€ infer_question_answer.pyï¼šè§†è§‰é—®ç­”æ¨ç†ç¨‹åº
â”‚   â”œâ”€â”€ infer_image_identification.pyï¼šå›¾åƒè¯†åˆ«æ¨ç†ç¨‹åº
â”‚   â”œâ”€â”€ infer_image_explanation.pyï¼šå›¾åƒè§£é‡Šæ¨ç†ç¨‹åº
â”‚   â””â”€â”€ pipeline.pyï¼šæµæ°´çº¿æ–¹æ³•æ¨ç†ç¨‹åº
â”œâ”€â”€ oursï¼šæ–¹æ³•ç¨‹åº
â”‚   â”œâ”€â”€ database_construct.pyï¼šçŸ¥è¯†åº“æ„å»ºç¨‹åº
â”‚   â”œâ”€â”€ retrieval_augment_generation.pyï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆç¨‹åº
â”‚   â””â”€â”€ object_detection.pyï¼šç›®æ ‡æ£€æµ‹ç¨‹åº
â”œâ”€â”€ toolsï¼šå·¥å…·ç¨‹åº
â”‚   â”œâ”€â”€ generate_vqa.pyï¼švqaæ•°æ®ç”Ÿæˆç¨‹åº
â”‚   â”œâ”€â”€ preprocess.pyï¼šæ•°æ®é¢„å¤„ç†ç¨‹åº
â”‚   â””â”€â”€ download.pyï¼šæ¨¡å‹ä¸‹è½½ç¨‹åº
â””â”€â”€ evaluateï¼šè¯„ä¼°ç¨‹åº
â”‚   â”œâ”€â”€ eval_image_caption.pyï¼šå›¾åƒæè¿°è¯„ä¼°ç¨‹åº
â”‚   â”œâ”€â”€ eval_question_answer.pyï¼šè§†è§‰é—®ç­”è¯„ç¨‹åº
â”‚   â””â”€â”€ eval_image_explanation.pyï¼šå›¾åƒè§£é‡Šè¯„ä¼°ç¨‹åº
â””â”€â”€ resultï¼šé¢„æµ‹ç»“æœ
â”‚   â”œâ”€â”€ image_captionï¼šå›¾åƒæè¿°ç»“æœï¼Œå¯¹åº”è®ºæ–‡è¡¨1
â”‚   â”œâ”€â”€ question_answerï¼šè§†è§‰é—®ç­”ç»“æœï¼Œå¯¹åº”è®ºæ–‡è¡¨1
â”‚   â”œâ”€â”€ pipeline_identificationï¼šå›¾åƒè¯†åˆ«ç»“æœï¼ˆæµæ°´çº¿ï¼‰ï¼Œå¯¹åº”è®ºæ–‡è¡¨2
â”‚   â”œâ”€â”€ pipeline_explanationï¼šå›¾åƒè§£é‡Šç»“æœï¼ˆæµæ°´çº¿ï¼‰ï¼Œå¯¹åº”è®ºæ–‡è¡¨2
â”‚   â”œâ”€â”€ image_identificationï¼šå›¾åƒè¯†åˆ«ç»“æœï¼ˆç«¯åˆ°ç«¯ï¼‰ï¼Œå¯¹åº”è®ºæ–‡è¡¨3
â”‚   â””â”€â”€ image_explanationï¼šå›¾åƒè§£é‡Šç»“æœï¼ˆç«¯åˆ°ç«¯ï¼‰ï¼Œå¯¹åº”è®ºæ–‡è¡¨3
â””â”€â”€ readme.mdï¼šè¯´æ˜æ–‡ä»¶
``` -->


```
â”œâ”€â”€ modelsï¼šcheckpoint files
â”œâ”€â”€ datasets
â”‚   â”œâ”€â”€ images
â”‚   â””â”€â”€ annotation.xlsx
â”œâ”€â”€ databaseï¼š
â”œâ”€â”€ results
â”‚   â”œâ”€â”€ image_captionï¼š
â”‚   â”œâ”€â”€ question_answerï¼š
â”‚   â”œâ”€â”€ image_identificationï¼š
â”‚   â””â”€â”€ image_explanationï¼š
â”œâ”€â”€ baselineï¼š
â”‚   â”œâ”€â”€ LLaVAï¼šLLaVA official project
â”‚   â”œâ”€â”€ mPLUG-Owlï¼šmPLUG-Owl official project
â”‚   â”œâ”€â”€ mPLUG-Owl2ï¼šmPLUG-Owl2 official project
â”‚   â”œâ”€â”€ Otterï¼šOtter official project
â”‚   â”œâ”€â”€ openflamingoï¼šopenflamingo official project
â”‚   â”œâ”€â”€ MICï¼šMMICL official project
â”‚   â”œâ”€â”€ llamaï¼šllama official project
â”‚   â”œâ”€â”€ FastChatï¼švicuna official project
â”‚   â”œâ”€â”€ demo
â”‚   â”œâ”€â”€ infer_image_caption.py
â”‚   â”œâ”€â”€ infer_question_answer.py
â”‚   â”œâ”€â”€ infer_image_identification.py
â”‚   â”œâ”€â”€ infer_image_explanation.py
â”‚   â””â”€â”€ pipeline.py
â”œâ”€â”€ ours
â”‚   â”œâ”€â”€ GLIP: GLIP official project 
â”‚   â”œâ”€â”€ database_construct.py
â”‚   â”œâ”€â”€ retrieval_augment_generation.py
â”‚   â””â”€â”€ object_detection.py
â”œâ”€â”€ tools
â”‚   â”œâ”€â”€ generate_vqa.py
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â””â”€â”€ download.py
â””â”€â”€ evaluate
â”‚   â”œâ”€â”€ eval_image_caption.py
â”‚   â”œâ”€â”€ eval_question_answer.py
â”‚   â”œâ”€â”€ eval_question_answer.py
â”‚   â”œâ”€â”€ eval_pipeline.py
â”‚   â””â”€â”€ eval_image_explanation.py
â””â”€â”€ result
â”‚   â”œâ”€â”€ image_caption:table-1
â”‚   â”œâ”€â”€ question_answer:table-1
â”‚   â”œâ”€â”€ pipeline_identification:table-2
â”‚   â”œâ”€â”€ pipeline_explanation:table-2
â”‚   â”œâ”€â”€ image_identification:table-3
â”‚   â””â”€â”€ image_explanation:table-3
â””â”€â”€ readme.md
```

# 2. Run

## (1) checkpoint download
```shell
export HF_ENDPOINT=https://hf-mirror.com
cd main
python download.py
```

## (2) data preprocess
```shell
python preprocess.py -task caption
python preprocess.py -task explanation
python preprocess.py -task vqa
```

## (3) environmental installation
BLIP series, mplug_owl series, llava series, etc. (refer to the environmental configuration of the official GitHub project)

```shell
conda activate blip
conda activate mplug_owl
conda activate mplug_owl2
conda activate llava
conda activate llama
conda activate vicuna
conda activate cfr
```

## (4) baseline inference

[Note:] The experimental test inputs for the four tasks vary slightly.

- <u>Image Captioning</u> and <u>Visual Question Answering</u> are tested on all images, including positive and negative samples. Only zero-shot setting is applied.
- <u>Image Recognition</u> is tested on all images. In the few-shot setting, in addition to reading the test image, it is also necessary to read 2 random samples (which may be positive or negative) from the same knowledge background as the test image; in the CoCoT setting, in addition to reading the test sample, it is also necessary to read the corresponding 1 opposite sample.
- <u>Image Explanation</u> is tested on negative sample images. In the few-shot setting, in addition to reading the test image, it is also necessary to read 2 random samples (which may be positive or negative) from the same knowledge background as the test image; in the CoCoT setting, in addition to reading the test sample, it is also necessary to read the corresponding 1 opposite sample (positive sample).
- GPT-4V is an exception. Post-processing of the results from <u>Image Explanation</u> is performed to obtain the results for <u>Image Recognition</u>, therefore its <u>Image Recognition</u> is also tested on negative samples.
- In the pipeline method, under the few-shot setting, the samples selected are two samples drawn from the entire dataset (which may be positive, negative, or from other knowledge backgrounds).


1. image caption inference
```shell
python infer_image_caption.py -model BLIP-Base
```
2. VQA inference
```shell
python infer_question_answer.py -model BLIP-Base
```
3. image identification inference
```shell
python infer_image_identification.py -model BLIP2-XL -setting z
```
4. image explanation inference
```shell
python infer_image_explanation.py -model BLIP2-XL -setting z
```
5. pipeline method inference
```shell
python pipeline.py -model LLaMA-2-7B -setting z -withCoT n
```

baseline models for image captionï¼š
| model | checkpoint file |
|:------ |:-------|
| BLIP-Base | ./models/blip-image-captioning-base |
| BLIP2-XL | ./models/blip2-flan-t5-xl |
| BLIP2-XXL | ./models/blip2-flan-t5-xxl |
| InstructBLIP-XL | ./models/instructblip-flan-t5-xl |
| InstructBLIP-XXL | ./models/instructblip-flan-t5-xl |
| mPLUG-owl-7B | ./models/mplug-owl-llama-7b |
| mPLUG-owl2-7B | ./models/mplug-owl2-llama-7b |
| LLaVA-1.5-7B | ./models/llava-v1.5-7b |
| LLaVA-1.6-7B | ./models/llava-v1.6-vicuna-7b |

baseline models for VQAï¼š
| model | checkpoint file |
|:------ |:-------|
| BLIP-Base | ./models/blip-vqa-base |
| BLIP2-XL | ./models/blip2-flan-t5-xl |
| BLIP2-XXL | ./models/blip2-flan-t5-xxl |
| InstructBLIP-XL | ./models/instructblip-flan-t5-xl |
| InstructBLIP-XXL | ./models/instructblip-flan-t5-xl |
| mPLUG-owl-7B | ./models/mplug-owl-llama-7b |
| mPLUG-owl2-7B | ./models/mplug-owl-llama2-7b |
| LLaVA-1.5-7B | ./models/llava-v1.5-7b |
| LLaVA-1.6-7B | ./models/llava-v1.6-vicuna-7b |

baseline models for image indentification and explanation
| model | checkpoint file | setting |
|:------ |:-------|:-------|
| BLIP2-XL | ./models/blip2-flan-t5-xl | zero-shot |
| BLIP2-XXL | ./models/blip2-flan-t5-xxl | zero-shot |
| InstructBLIP-XL | ./models/instructblip-flan-t5-xl | zero-shot |
| InstructBLIP-XXL | ./models/instructblip-flan-t5-xl | zero-shot |
| mPLUG-owl-7B | ./models/mplug-owl-llama-7b | zero-shot |
| mPLUG-owl2-7B | ./models/mplug-owl2-llama-7b | zero-shot |
| LLaVA-1.5-7B | ./models/llava-v1.5-7b | zero-shot |
| LLaVA-1.6-7B | ./models/llava-v1.6-vicuna-7b | zero-shot |
| MMICL | ./models/MMICL-Instructblip-T5-xl  | few-shot, CoCoT |
| OpenFlamingo | ./models/OpenFlamingo-3B-vitl-mpt1b | few-shot, CoCoT |
| Otter-7B | ./models/OTTER-Image-LLaMA7B-LA-InContext | few-shot, CoCoT |
| GEMINI | coming soon... | few-shot, CoCoT |
| GPT-4V | sk-XXXXXXXXXXXXXXXXXXXXX | few-shot, CoCoT |


LLM models for pipeline method
| model | checkpoint file |
|:------ |:-------|
| llama-2-7b | ./models/Llama-2-7b-hf |
| llama-2-13b | ./models/Llama-2-13b-hf |
| vicuna-1.5-7b  | ./models/vicuna-7b-v1.5 |
| vicuna-1.5-7b | ./models/vicuna-13b-v1.5 |
| GPT-3.5 | sk-XXXXXXXXXXXXXXXXXXXXX |

## (5) ours method inference
coming soon...

# 3. performance evaluation
1. image caption evaluation
```shell
python evaluate/eval_image_caption.py
```
2. VQA evaluation
```shell
python evaluate/eval_question_answer.py
```
3. image identification evaluation
```shell
python evaluate/eval_identification.py
```
4. image explanation evaluation
```shell
python evaluate/eval_image_explanation.py
```
